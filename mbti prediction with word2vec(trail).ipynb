{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "929ef536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Collecting FuzzyTM>=0.4.0\n",
      "  Downloading FuzzyTM-2.0.5-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.10.0)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Collecting pyfume\n",
      "  Downloading pyFUME-0.2.25-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 67.1/67.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Collecting simpful\n",
      "  Downloading simpful-2.11.0-py3-none-any.whl (32 kB)\n",
      "Collecting fst-pso\n",
      "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Collecting miniful\n",
      "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: fst-pso, miniful\n",
      "  Building wheel for fst-pso (setup.py): started\n",
      "  Building wheel for fst-pso (setup.py): finished with status 'done'\n",
      "  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20448 sha256=88aee6637f468ee5a32fad43d7609f4bb1998ee104ee35a2151b6cbc1f246bfe\n",
      "  Stored in directory: c:\\users\\adarsh\\appdata\\local\\pip\\cache\\wheels\\01\\02\\ee\\df0699282986903a384b69aab4413af9efd26b3612b5dccc9e\n",
      "  Building wheel for miniful (setup.py): started\n",
      "  Building wheel for miniful (setup.py): finished with status 'done'\n",
      "  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3522 sha256=03885df31fd18e8993b542a239b9c2016a15a6014d9ab344bc3af602a38201d8\n",
      "  Stored in directory: c:\\users\\adarsh\\appdata\\local\\pip\\cache\\wheels\\43\\aa\\48\\5c66b931ff013ad19774081aa19656637af5c0cc33b5494b30\n",
      "Successfully built fst-pso miniful\n",
      "Installing collected packages: simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
      "Successfully installed FuzzyTM-2.0.5 fst-pso-1.8.1 miniful-0.0.6 pyfume-0.2.25 simpful-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717ca0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#from pywaffle import Waffle\n",
    "#from catboost import CatBoostClassifier, pool\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c81956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Adarsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /kaggle/working/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "# Check if WordNet data is available and download if not\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/nltk_data')\n",
    "\n",
    "# Update the NLTK data path to include the WordNet data\n",
    "nltk.data.path.append('/kaggle/working/nltk_data')\n",
    "\n",
    "# Check if stopwords data is available and download if not\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', download_dir='/kaggle/working/nltk_data')\n",
    "\n",
    "# Update the NLTK data path to include the stopwords data\n",
    "nltk.data.path.append('/kaggle/working/nltk_data')\n",
    "\n",
    "\n",
    "    \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import squarify\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e714b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "sy =wordnet.synsets('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9d164b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset name: extrovert.n.01\n",
      "Definition: (psychology) a person concerned more with practical realities than with inner thoughts and feelings\n",
      "Examples: []\n",
      "\n",
      "Synset name: extrovert.s.01\n",
      "Definition: being concerned with the social and physical environment\n",
      "Examples: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "synsets = wordnet.synsets('extrovert')\n",
    "\n",
    "# Print synset information\n",
    "for synset in synsets:\n",
    "    print(f\"Synset name: {synset.name()}\")\n",
    "    print(f\"Definition: {synset.definition()}\")\n",
    "    print(f\"Examples: {synset.examples()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343eca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(mode, df, col_name, class_names, count):\n",
    "    \"\"\"\n",
    "    Samples the dataframe based on the mode, given class names and count.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    mode: str\n",
    "        Either 'under' or 'over'\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame to be undersampled\n",
    "    col_name : str\n",
    "        Name of the column containing the target class\n",
    "    class_names : list\n",
    "        List of target class names to be undersampled\n",
    "    count : int\n",
    "        Number of samples to be kept for each class\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Undersampled DataFrame or Oversampled\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for class_name in class_names:\n",
    "        if mode == 'under':\n",
    "            df_class = df[df[col_name] == class_name]\n",
    "            if len(df_class) > count:\n",
    "                df_class = df_class.sample(count, replace=False)\n",
    "            dfs.append(df_class)\n",
    "        elif mode == 'over':\n",
    "            df_class = df[df[col_name] == class_name]\n",
    "            if len(df_class) < count:\n",
    "                df_class = df_class.sample(count, replace=True)\n",
    "            dfs.append(df_class)\n",
    "    dfs = pd.concat(dfs)\n",
    "    df_x = df[~df[col_name].isin(class_names)]\n",
    "    return pd.concat([dfs,df_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9eb2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train(train_data):\n",
    "    \n",
    "    X_train = np.vstack(np.array(train_data.posts))\n",
    "    y_train = np.array(train_data.type)\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fec929f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def clear_text(df):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_posts = []\n",
    "    for post in df.posts:\n",
    "        # Remove special characters and digits\n",
    "        post = re.sub(r\"[^a-zA-Z]\", \" \", post)\n",
    "        # Tokenize and lemmatize words\n",
    "        words = word_tokenize(post.lower())\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        cleaned_posts.append(\" \".join(words))\n",
    "    return cleaned_posts, len(cleaned_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a786e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, size):\n",
    "    # Cleaning The Data\n",
    "    df.posts, length = clear_text(df)\n",
    "\n",
    "    # Splitting into train & test\n",
    "    print(\"Splitting into train & test\")\n",
    "    train_data, test_data = train_test_split(df, test_size=size, random_state=0, stratify=df.type)\n",
    "\n",
    "    # Applying Word2Vec\n",
    "    print(\"Applying Word2Vec\")\n",
    "    tokenized_posts = [word_tokenize(post) for post in train_data.posts]\n",
    "    model = Word2Vec(tokenized_posts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    # Getting the average word embeddings for each post in the training set\n",
    "    train_post = [sum([model.wv[word] for word in words]) / len(words) for words in tokenized_posts]\n",
    "    train_post = np.array(train_post)\n",
    "\n",
    "    # Applying the same Word2Vec model to the test set\n",
    "    tokenized_test_posts = [word_tokenize(post) for post in test_data.posts]\n",
    "    test_post = [sum([model.wv[word] for word in words]) / len(words) for words in tokenized_test_posts]\n",
    "    test_post = np.array(test_post)\n",
    "\n",
    "    # Label Encoding the classes as 0, 1, 2, 3......\n",
    "    print(\"Label Encoding the classes\")\n",
    "    target_encoder = LabelEncoder()\n",
    "\n",
    "    # Getting the final train and test\n",
    "    print(\"Getting the final train and test\")\n",
    "    train_target = target_encoder.fit_transform(train_data.type)\n",
    "    test_target = target_encoder.fit_transform(test_data.type)\n",
    "    print(target_encoder.classes_)\n",
    "    return train_post, test_post, train_target, test_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36d393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
